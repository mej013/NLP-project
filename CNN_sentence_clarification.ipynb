{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package             Version    \n",
      "------------------- -----------\n",
      "absl-py             0.5.0      \n",
      "alembic             1.0.1      \n",
      "asn1crypto          0.24.0     \n",
      "astor               0.7.1      \n",
      "async-generator     1.10       \n",
      "backcall            0.1.0      \n",
      "bleach              3.0.0      \n",
      "certifi             2018.8.24  \n",
      "cffi                1.11.5     \n",
      "chardet             3.0.4      \n",
      "conda               4.5.11     \n",
      "cryptography        2.3.1      \n",
      "cycler              0.10.0     \n",
      "cymem               1.31.2     \n",
      "cytoolz             0.9.0.1    \n",
      "decorator           4.3.0      \n",
      "defusedxml          0.5.0      \n",
      "dill                0.2.8.2    \n",
      "entrypoints         0.2.3      \n",
      "gast                0.2.0      \n",
      "GPUtil              1.3.0      \n",
      "grpcio              1.15.0     \n",
      "h5py                2.8.0      \n",
      "idna                2.7        \n",
      "ipykernel           5.0.0      \n",
      "ipython             6.5.0      \n",
      "ipython-genutils    0.2.0      \n",
      "ipywidgets          7.4.2      \n",
      "jedi                0.13.1     \n",
      "Jinja2              2.10       \n",
      "jsonschema          2.6.0      \n",
      "jupyter             1.0.0      \n",
      "jupyter-client      5.2.3      \n",
      "jupyter-console     5.2.0      \n",
      "jupyter-core        4.4.0      \n",
      "jupyterhub          0.9.4      \n",
      "Keras               2.2.4      \n",
      "Keras-Applications  1.0.6      \n",
      "Keras-Preprocessing 1.0.5      \n",
      "kiwisolver          1.0.1      \n",
      "Mako                1.0.7      \n",
      "Markdown            3.0.1      \n",
      "MarkupSafe          1.0        \n",
      "matplotlib          3.0.0      \n",
      "mistune             0.8.3      \n",
      "mkl-fft             1.0.6      \n",
      "mkl-random          1.0.1      \n",
      "msgpack             0.5.6      \n",
      "msgpack-numpy       0.4.3.2    \n",
      "murmurhash          0.28.0     \n",
      "nbconvert           5.4.0      \n",
      "nbformat            4.4.0      \n",
      "nbresuse            0.2.0      \n",
      "networkx            2.2        \n",
      "nltk                3.3        \n",
      "nose                1.3.7      \n",
      "notebook            5.7.0      \n",
      "numpy               1.15.2     \n",
      "olefile             0.46       \n",
      "pamela              0.3.0      \n",
      "pandas              0.23.4     \n",
      "pandocfilters       1.4.2      \n",
      "parso               0.3.1      \n",
      "pexpect             4.6.0      \n",
      "pickleshare         0.7.5      \n",
      "Pillow              5.3.0      \n",
      "pip                 10.0.1     \n",
      "plac                0.9.6      \n",
      "preshed             1.0.1      \n",
      "prometheus-client   0.4.0      \n",
      "prompt-toolkit      1.0.15     \n",
      "protobuf            3.6.1      \n",
      "psutil              5.4.7      \n",
      "ptyprocess          0.6.0      \n",
      "pycosat             0.6.3      \n",
      "pycparser           2.19       \n",
      "Pygments            2.2.0      \n",
      "pygpu               0.7.6      \n",
      "pyOpenSSL           18.0.0     \n",
      "pyparsing           2.2.2      \n",
      "PySocks             1.6.8      \n",
      "python-dateutil     2.7.3      \n",
      "python-editor       1.0.3      \n",
      "python-oauth2       1.1.0      \n",
      "pytz                2018.5     \n",
      "PyYAML              3.13       \n",
      "pyzmq               17.1.2     \n",
      "qtconsole           4.4.1      \n",
      "regex               2018.7.11  \n",
      "requests            2.19.1     \n",
      "ruamel-yaml         0.15.46    \n",
      "scikit-learn        0.20.0     \n",
      "scipy               1.1.0      \n",
      "Send2Trash          1.5.0      \n",
      "setuptools          39.1.0     \n",
      "simplegeneric       0.8.1      \n",
      "six                 1.11.0     \n",
      "sklearn             0.0        \n",
      "spacy               2.0.12     \n",
      "SQLAlchemy          1.2.12     \n",
      "tensorboard         1.11.0     \n",
      "tensorflow          1.11.0     \n",
      "tensorflow-gpu      1.11.0     \n",
      "termcolor           1.1.0      \n",
      "terminado           0.8.1      \n",
      "testpath            0.4.2      \n",
      "Theano              1.0.3      \n",
      "thinc               6.10.3     \n",
      "toolz               0.9.0      \n",
      "torch               0.4.1.post2\n",
      "torchvision         0.2.1      \n",
      "tornado             5.1.1      \n",
      "tqdm                4.26.0     \n",
      "traitlets           4.3.2      \n",
      "ujson               1.35       \n",
      "urllib3             1.23       \n",
      "wcwidth             0.1.7      \n",
      "webencodings        0.5.1      \n",
      "Werkzeug            0.14.1     \n",
      "wheel               0.32.0     \n",
      "widgetsnbextension  3.4.2      \n",
      "wrapt               1.10.11    \n",
      "\u001b[33mYou are using pip version 10.0.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import random\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import keras\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.initializers import Constant\n",
    "from keras.models import Model\n",
    "from keras.layers import *\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1467810672, 'Mon Apr 06 22:19:49 PDT 2009', 'NO_QUERY', 'scotthamilton', \"is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!\"]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data.csv', encoding='ISO-8859-1')\n",
    "data = df.values.tolist()\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "master_data = []\n",
    "for l in data:\n",
    "    master_data.append([l[0], l[5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random.Random(73).shuffle(master_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 'thought the Summer Ball was awesome! Well done everyone ']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smaller_data = master_data[:50000]\n",
    "smaller_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['thought', 'the', 'Summer', 'Ball', 'was', 'awesome', '!', 'Well', 'done', 'everyone']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = TweetTokenizer()\n",
    "r = tokenizer.tokenize(smaller_data[0][1])\n",
    "print(r)\n",
    "counter = 0\n",
    "for l in smaller_data:\n",
    "    r = ' '.join([x  for x in tokenizer.tokenize(l[1]) if x[0] != '@'])\n",
    "    smaller_data[counter][1] = r\n",
    "    counter +=1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 'thought the Summer Ball was awesome ! Well done everyone']\n"
     ]
    }
   ],
   "source": [
    "print(smaller_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "252\n"
     ]
    }
   ],
   "source": [
    "max = 0\n",
    "for pair in smaller_data:\n",
    "    strLen = len(pair[1].split())\n",
    "    if strLen > max:\n",
    "        max = strLen\n",
    "print(max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "sequence_length = 252\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test set size 40000\n",
      "test set size 10000\n",
      "test set size 40000\n",
      "test set size 10000\n"
     ]
    }
   ],
   "source": [
    "max_features = 2000 # this is the number of words we care about\n",
    "\n",
    "text = [x[1] for x in smaller_data]\n",
    "tokenizer = Tokenizer(num_words=max_features, split=' ', oov_token='<unw>')\n",
    "tokenizer.fit_on_texts(text)\n",
    "\n",
    "# this takes our sentences and replaces each word with an integer\n",
    "X = tokenizer.texts_to_sequences(text)\n",
    "\n",
    "# we then pad the sequences so they're all the same length (sequence_length)\n",
    "X = pad_sequences(X, sequence_length)\n",
    "\n",
    "#print(label)\n",
    "y = []\n",
    "for label in smaller_data:\n",
    "    if label[0] == 0:\n",
    "        y.append([1,0,0,0,0])\n",
    "    else:\n",
    "        y.append([0,0,0,0,1])\n",
    "\n",
    "\n",
    "        \n",
    "X_train, X_test, Y_train, Y_test = np.array(X[:40000]), np.array(X[40000:50000]), np.array(y[:40000]), np.array(y[40000:50000])\n",
    "\n",
    "print(\"test set size \" + str(len(X_train)))\n",
    "print(\"test set size \" + str(len(X_test)))\n",
    "print(\"test set size \" + str(len(Y_train)))\n",
    "print(\"test set size \" + str(len(Y_test)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_dim = 200 # We uses 300 here\n",
    "num_filters = 100\n",
    "\n",
    "inputs = Input(shape=(sequence_length,), dtype='int32')\n",
    "\n",
    "# use a random embedding for the text\n",
    "embedding_layer = Embedding(input_dim=max_features, output_dim=embedding_dim, input_length=sequence_length)(inputs)\n",
    "\n",
    "reshape = Reshape((sequence_length, embedding_dim, 1))(embedding_layer)\n",
    "\n",
    "# Note the relu activation which we specifically mentions\n",
    "# He also uses an l2 constraint of 3\n",
    "# Also, note that the convolution window acts on the whole 200 dimensions - that's important\n",
    "conv_0 = Conv2D(num_filters, kernel_size=(3, embedding_dim), activation='relu', kernel_regularizer=regularizers.l2(3))(reshape)\n",
    "conv_1 = Conv2D(num_filters, kernel_size=(4, embedding_dim), activation='relu', kernel_regularizer=regularizers.l2(3))(reshape)\n",
    "conv_2 = Conv2D(num_filters, kernel_size=(5, embedding_dim), activation='relu', kernel_regularizer=regularizers.l2(3))(reshape)\n",
    "\n",
    "# perform max pooling on each of the convoluations\n",
    "maxpool_0 = MaxPool2D(pool_size=(sequence_length - 3 + 1, 1), strides=(1,1), padding='valid')(conv_0)\n",
    "maxpool_1 = MaxPool2D(pool_size=(sequence_length - 4 + 1, 1), strides=(1,1), padding='valid')(conv_1)\n",
    "maxpool_2 = MaxPool2D(pool_size=(sequence_length - 5 + 1, 1), strides=(1,1), padding='valid')(conv_2)\n",
    "\n",
    "# concat and flatten\n",
    "concatenated_tensor = Concatenate(axis=1)([maxpool_0, maxpool_1, maxpool_2])\n",
    "flatten = Flatten()(concatenated_tensor)\n",
    "\n",
    "# do dropout and predict\n",
    "dropout = Dropout(0.5)(flatten)\n",
    "output = Dense(5, activation='softmax')(dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 252)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 252, 200)     400000      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 252, 200, 1)  0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 250, 1, 100)  60100       reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 249, 1, 100)  80100       reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 248, 1, 100)  100100      reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 1, 1, 100)    0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 1, 1, 100)    0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 1, 1, 100)    0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 3, 1, 100)    0           max_pooling2d_1[0][0]            \n",
      "                                                                 max_pooling2d_2[0][0]            \n",
      "                                                                 max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 300)          0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 300)          0           flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 5)            1505        dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 641,805\n",
      "Trainable params: 641,805\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=inputs, outputs=output)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36000 samples, validate on 4000 samples\n",
      "Epoch 1/15\n",
      "36000/36000 [==============================] - 21s 593us/step - loss: 0.8184 - acc: 0.4993 - val_loss: 0.7038 - val_acc: 0.5040\n",
      "Epoch 2/15\n",
      "36000/36000 [==============================] - 13s 370us/step - loss: 0.7157 - acc: 0.5716 - val_loss: 0.6890 - val_acc: 0.6580\n",
      "Epoch 3/15\n",
      "36000/36000 [==============================] - 13s 370us/step - loss: 0.7112 - acc: 0.6984 - val_loss: 0.7382 - val_acc: 0.7065\n",
      "Epoch 4/15\n",
      "36000/36000 [==============================] - 13s 364us/step - loss: 0.7064 - acc: 0.7314 - val_loss: 0.6668 - val_acc: 0.7445\n",
      "Epoch 5/15\n",
      "36000/36000 [==============================] - 13s 361us/step - loss: 0.7006 - acc: 0.7448 - val_loss: 0.7207 - val_acc: 0.7372\n",
      "Epoch 6/15\n",
      "36000/36000 [==============================] - 13s 357us/step - loss: 0.6926 - acc: 0.7539 - val_loss: 0.6998 - val_acc: 0.7542\n",
      "Epoch 7/15\n",
      "36000/36000 [==============================] - 13s 360us/step - loss: 0.6950 - acc: 0.7542 - val_loss: 0.6703 - val_acc: 0.7475\n",
      "Epoch 8/15\n",
      "36000/36000 [==============================] - 13s 358us/step - loss: 0.6933 - acc: 0.7584 - val_loss: 0.6694 - val_acc: 0.7478\n",
      "Epoch 9/15\n",
      "36000/36000 [==============================] - 13s 360us/step - loss: 0.6981 - acc: 0.7607 - val_loss: 0.7522 - val_acc: 0.7350\n",
      "Epoch 10/15\n",
      "36000/36000 [==============================] - 13s 364us/step - loss: 0.7025 - acc: 0.7607 - val_loss: 0.6802 - val_acc: 0.7502\n",
      "Epoch 11/15\n",
      "36000/36000 [==============================] - 13s 364us/step - loss: 0.6992 - acc: 0.7643 - val_loss: 0.6996 - val_acc: 0.7510\n",
      "Epoch 12/15\n",
      "36000/36000 [==============================] - 13s 367us/step - loss: 0.7055 - acc: 0.7639 - val_loss: 0.6879 - val_acc: 0.7548\n",
      "Epoch 13/15\n",
      "36000/36000 [==============================] - 13s 361us/step - loss: 0.7069 - acc: 0.7670 - val_loss: 0.7135 - val_acc: 0.7532\n",
      "Epoch 14/15\n",
      "36000/36000 [==============================] - 13s 364us/step - loss: 0.7073 - acc: 0.7646 - val_loss: 0.7592 - val_acc: 0.7242\n",
      "Epoch 15/15\n",
      "36000/36000 [==============================] - 13s 360us/step - loss: 0.7102 - acc: 0.7635 - val_loss: 0.7360 - val_acc: 0.7428\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32 \n",
    "history = model.fit(X_train, Y_train, epochs=15, batch_size=batch_size, verbose=1, validation_split=0.1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_hat = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7536"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(list(map(lambda x: np.argmax(x), Y_test)), list(map(lambda x: np.argmax(x), y_hat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3318, 1586],\n",
       "       [ 878, 4218]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(list(map(lambda x: np.argmax(x), Y_test)), list(map(lambda x: np.argmax(x), y_hat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"CnnModel.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"CnnModel.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = model.predict(np.array([X_train[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.01171605e-01 1.11224425e-08 1.06154170e-08 9.53598356e-09\n",
      "  8.98828387e-01]]\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(Y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

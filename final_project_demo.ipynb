{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/JMX/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import string\n",
    "import numpy as np\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import model_from_json\n",
    "from sklearn.externals import joblib\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['thought', 'the', 'Summer', 'Ball', 'was', 'awesome', '!', 'Well', 'done', 'everyone']\n"
     ]
    }
   ],
   "source": [
    "# data processing for RNN model\n",
    "file = pd.read_csv(\"./data.csv\", encoding='ISO-8859-1')\n",
    "data = file.values.tolist()\n",
    "master_data = []\n",
    "for l in data:\n",
    "    master_data.append([l[0], l[5]])\n",
    "random.Random(73).shuffle(master_data)\n",
    "smaller_data = master_data[:50000]\n",
    "tokenizer = TweetTokenizer()\n",
    "r = tokenizer.tokenize(smaller_data[0][1])\n",
    "print(r)\n",
    "counter = 0\n",
    "for l in smaller_data:\n",
    "    r = ' '.join([x  for x in tokenizer.tokenize(l[1]) if x[0] != '@'])\n",
    "    smaller_data[counter][1] = r\n",
    "    counter +=1\n",
    "texts = [x[1] for x in smaller_data]\n",
    "Y = []\n",
    "for l in smaller_data:\n",
    "    if(l[0] == 0 ):\n",
    "        Y.append([0, 1])\n",
    "    else:\n",
    "        Y.append([1, 0])\n",
    "max_fatures = 2000\n",
    "feature_tokenizer = Tokenizer(num_words=max_fatures, split=' ')\n",
    "feature_tokenizer.fit_on_texts(texts)\n",
    "X = feature_tokenizer.texts_to_sequences(texts)\n",
    "X = pad_sequences(X)\n",
    "\n",
    "embed_dim = 128\n",
    "lstm_out = 196  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_tokenizer_cnn = Tokenizer(num_words=max_fatures, split=' ', oov_token='<unw>')\n",
    "feature_tokenizer_cnn.fit_on_texts(texts)\n",
    "X_cnn = feature_tokenizer_cnn.texts_to_sequences(texts)\n",
    "X_cnn = pad_sequences(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 'thought the Summer Ball was awesome ! Well done everyone']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM model and baseline model data processing \n",
    "smaller_data_punc = []\n",
    "smaller_data = master_data[:300000]\n",
    "# with punctuation \n",
    "counter = 0\n",
    "for l in master_data[:300000]:\n",
    "    r = ''.join([c for c in l[1].lower() if c !='@'])\n",
    "    if(l[0] == 0):\n",
    "        smaller_data_punc.append([0, r])\n",
    "    else:\n",
    "        smaller_data_punc.append([1, r])\n",
    "    counter +=1\n",
    "    \n",
    "train_text = [x[1] for x in smaller_data[:200000]]\n",
    "\n",
    "wordCount = defaultdict(int)\n",
    "for l in train_text:\n",
    "    l = l.split()\n",
    "    if(len(l) == 0):\n",
    "        continue\n",
    "    wordCount[l[0]] += 1\n",
    "    for i in range(1, len(l)):\n",
    "        wordCount[l[i]] += 1\n",
    "\n",
    "counts = [(wordCount[w], w) for w in wordCount]\n",
    "counts.sort()\n",
    "counts.reverse()\n",
    "\n",
    "words = [x[1] for x in counts[:2000]]\n",
    "\n",
    "\n",
    "unigram_frequency = defaultdict(float)\n",
    "total_count = sum([i[0] for i in counts[:2000]])\n",
    "for i in counts[:2000]:\n",
    "    unigram_frequency[i[1]] = i[0]/total_count \n",
    "    \n",
    "tweet = TweetTokenizer()\n",
    "\n",
    "\n",
    "wordCountP = defaultdict(int)\n",
    "for d in smaller_data_punc:\n",
    "    review = d[1]\n",
    "    tokens = tweet.tokenize(review)\n",
    "    for w in tokens:\n",
    "        wordCountP[w] += 1\n",
    "\n",
    "countsP = [(wordCountP[w], w) for w in wordCountP]\n",
    "countsP.sort()\n",
    "countsP.reverse()\n",
    "\n",
    "wordsP = [x[1] for x in countsP[:2000]]\n",
    "\n",
    "wordIdP = dict(zip(wordsP, range(len(wordsP))))\n",
    "wordSetP = set(wordsP)\n",
    "\n",
    "\n",
    "# Calculating the most 2000 frequenct words frequncy in probalility in positive sentiment text\n",
    "unigram_frequency_pos = defaultdict(float)\n",
    "train_text_pos = [x[1] for x in smaller_data[:200000] if x[0] == 4]\n",
    "for l in train_text_pos:\n",
    "    l = l.split()\n",
    "    if(len(l) == 0):\n",
    "        continue\n",
    "    for w in l:\n",
    "        if w in words:\n",
    "            unigram_frequency_pos[w] += 1\n",
    "\n",
    "\n",
    "total_count = 0\n",
    "for w in unigram_frequency_pos:\n",
    "    total_count += unigram_frequency_pos[w]\n",
    "for w in unigram_frequency_pos:\n",
    "    unigram_frequency_pos[w] = unigram_frequency_pos[w]/total_count\n",
    "positive_words = []\n",
    "for w in unigram_frequency:\n",
    "    positive_words.append([unigram_frequency_pos[w] - unigram_frequency[w], w])\n",
    "positive_words.sort()\n",
    "positive_words.reverse()\n",
    "pos_words = [x[1] for x in positive_words][:200]\n",
    "\n",
    "# Calculating the most 2000 frequenct words frequncy in probalility in negtive sentiment text\n",
    "unigram_frequency_neg = defaultdict(float)\n",
    "train_text_neg = [x[1] for x in smaller_data[:200000] if x[0] == 0]\n",
    "for l in train_text_neg:\n",
    "    l = l.split()\n",
    "    if(len(l) == 0):\n",
    "        continue\n",
    "    for w in l:\n",
    "        if w in words:\n",
    "            unigram_frequency_neg[w] += 1\n",
    "total_count = 0\n",
    "for w in unigram_frequency_neg:\n",
    "    total_count += unigram_frequency_neg[w]\n",
    "for w in unigram_frequency_neg:\n",
    "    unigram_frequency_neg[w] = unigram_frequency_neg[w]/total_count\n",
    "negative_words = []\n",
    "for w in unigram_frequency:\n",
    "    negative_words.append([unigram_frequency_neg[w] - unigram_frequency[w], w])\n",
    "negative_words.sort()\n",
    "negative_words.reverse()\n",
    "neg_words = [x[1] for x in negative_words][:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['you',\n",
       " '!',\n",
       " 'a',\n",
       " 'the',\n",
       " 'for',\n",
       " 'love',\n",
       " '-',\n",
       " 'your',\n",
       " 'good',\n",
       " 'with',\n",
       " ',',\n",
       " 'great',\n",
       " 'are',\n",
       " 'thanks',\n",
       " 'Thanks',\n",
       " 'and',\n",
       " 'new',\n",
       " 'some',\n",
       " 'Good',\n",
       " 'can',\n",
       " 'You',\n",
       " 'of',\n",
       " 'happy',\n",
       " 'nice',\n",
       " '?',\n",
       " 'thank',\n",
       " 'haha',\n",
       " \"you're\",\n",
       " 'u',\n",
       " 'fun',\n",
       " 'will',\n",
       " 'Thank',\n",
       " 'see',\n",
       " 'awesome',\n",
       " 'on',\n",
       " 'wait',\n",
       " 'lol',\n",
       " '\"',\n",
       " 'watching',\n",
       " 'Happy',\n",
       " 'if',\n",
       " 'best',\n",
       " 'amazing',\n",
       " \"I'll\",\n",
       " 'then',\n",
       " 'we',\n",
       " 'follow',\n",
       " '&amp;',\n",
       " 'should',\n",
       " 'glad',\n",
       " 'LOL',\n",
       " 'cool',\n",
       " 'day',\n",
       " 'morning',\n",
       " 'Hey',\n",
       " 'excited',\n",
       " 'from',\n",
       " 'everyone',\n",
       " 'Hope',\n",
       " 'Love',\n",
       " 'song',\n",
       " 'hey',\n",
       " 'twitter',\n",
       " 'welcome',\n",
       " 'as',\n",
       " 'first',\n",
       " 'time',\n",
       " 'beautiful',\n",
       " 'what',\n",
       " 'ur',\n",
       " 'check',\n",
       " 'Have',\n",
       " \"that's\",\n",
       " 'sure',\n",
       " 'The',\n",
       " 'cute',\n",
       " 'that',\n",
       " '&lt;3',\n",
       " 'movie',\n",
       " 'birthday',\n",
       " 'sweet',\n",
       " 'ya',\n",
       " 'Just',\n",
       " 'hope',\n",
       " ':',\n",
       " 'or',\n",
       " 'finally',\n",
       " 'listening',\n",
       " 'enjoy',\n",
       " 'ready',\n",
       " 'night',\n",
       " 'better',\n",
       " 'pretty',\n",
       " ')',\n",
       " 'funny',\n",
       " 'yes',\n",
       " 'made',\n",
       " 'lovely',\n",
       " 'you!',\n",
       " 'guys',\n",
       " 'yay',\n",
       " 'forward',\n",
       " 'us',\n",
       " 'sounds',\n",
       " 'Morning',\n",
       " 'it!',\n",
       " 'just',\n",
       " 'Get',\n",
       " 'tweet',\n",
       " 'music',\n",
       " 'Hi',\n",
       " \"You're\",\n",
       " 'add',\n",
       " 'try',\n",
       " ';)',\n",
       " 'If',\n",
       " 'wonderful',\n",
       " 'how',\n",
       " 'followers',\n",
       " 'Great',\n",
       " 'hehe',\n",
       " 'look',\n",
       " 'say',\n",
       " 'following',\n",
       " 'x',\n",
       " 'later',\n",
       " 'Nice',\n",
       " 'well',\n",
       " \"'\",\n",
       " 'Watching',\n",
       " 'way',\n",
       " 'LOVE',\n",
       " 'We',\n",
       " 'Haha',\n",
       " 'very',\n",
       " 'who',\n",
       " 'Glad',\n",
       " 'Twitter',\n",
       " 'hahaha',\n",
       " 'Going',\n",
       " 'A',\n",
       " '&',\n",
       " 'more',\n",
       " 'all',\n",
       " 'one',\n",
       " 'New',\n",
       " 'always',\n",
       " 'smile',\n",
       " 'our',\n",
       " 'Yay',\n",
       " 'done',\n",
       " 'using',\n",
       " 'favorite',\n",
       " \"It's\",\n",
       " '#followfriday',\n",
       " 'hello',\n",
       " 'name',\n",
       " 'too!',\n",
       " 'And',\n",
       " 'yeah',\n",
       " 'hi',\n",
       " 'video',\n",
       " 'blog',\n",
       " 'free',\n",
       " \"you'll\",\n",
       " 'an',\n",
       " \"That's\",\n",
       " 'ha',\n",
       " 'luck',\n",
       " 'than',\n",
       " 'enjoying',\n",
       " 'day!',\n",
       " 'course',\n",
       " 'Day',\n",
       " 'Your',\n",
       " 'reading',\n",
       " 'also',\n",
       " 'maybe',\n",
       " 'How',\n",
       " 'world',\n",
       " 'loving',\n",
       " 'God',\n",
       " 'Thanks!',\n",
       " 'Welcome',\n",
       " 'Congrats',\n",
       " 'thanks!',\n",
       " 'soon',\n",
       " 'dinner',\n",
       " 'those',\n",
       " 'friends',\n",
       " 'See',\n",
       " 'you?',\n",
       " 'proud',\n",
       " '100',\n",
       " 'goodnight',\n",
       " 'album',\n",
       " 'tell',\n",
       " 'Yes',\n",
       " '..',\n",
       " '<3']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM Model feature function\n",
    "def feature3(datum):\n",
    "    feat = [0]*len(wordsP)\n",
    "    review = datum[1]\n",
    "    tokens = tweet.tokenize(review)\n",
    "    for w in tokens:\n",
    "        if w in wordsP:\n",
    "            feat[wordIdP[w]] += 1\n",
    "    feat.append(1) #offset\n",
    "    return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline Model Feature Function\n",
    "punctuation = set(string.punctuation)\n",
    "def baseFeature(datum):\n",
    "    positive_count = 0\n",
    "    negative_count = 0\n",
    "    r = ''.join([c for c in datum.lower() if not c in punctuation])\n",
    "    for w in r.split():\n",
    "        if w in pos_words:\n",
    "            positive_count += 1\n",
    "        if w in neg_words:\n",
    "            negative_count += 1\n",
    "    print(\"Positive word counts: \", positive_count, \"Negative word counts: \", negative_count)\n",
    "    if(negative_count > positive_count):\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "# load json and create RNNmodel\n",
    "json_file = open('RnnModel.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_RnnModel = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_RnnModel.load_weights(\"RnnModel.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "# load json and create CNNmodel\n",
    "json_file = open('CnnModel.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_CnnModel = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_CnnModel.load_weights(\"CnnModel.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load SVM model \n",
    "clf = joblib.load('svm_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict using baseline model\n",
    "def predict_baseline(str_1):\n",
    "    base_feature = baseFeature(str_1)\n",
    "    if base_feature == 1:\n",
    "        print(\"It's a positive sentence\")\n",
    "    else:\n",
    "        print(\"It's a negative sentence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict using SVM model\n",
    "def predict_svm(str_1):\n",
    "    datum = [0,str_1]\n",
    "    feat = []\n",
    "    feat.append(feature3(datum))\n",
    "    if clf.predict(feat)[0] == 0:\n",
    "        print(\"It's a negative sentence\")\n",
    "    else:\n",
    "        print(\"It's a positive sentence\")\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict using LSTM_RNN model\n",
    "def predict_LSTM_RNN(str_1):\n",
    "    r = ' '.join([x  for x in tokenizer.tokenize(str_1) if x[0] != '@'])\n",
    "    max_string = ''.join('and ' for i in range(246))\n",
    "    test_x = feature_tokenizer.texts_to_sequences([r, max_string])\n",
    "    test_x = pad_sequences(test_x)\n",
    "    result = loaded_RnnModel.predict(np.array([test_x[0]]))\n",
    "    print(\"Positive prob: \", result[0][0], \"Negative prob: \", result[0][1])\n",
    "    if(result[0][0] > result[0][1]):\n",
    "        print(\"It's a positive sentence\")\n",
    "    else:\n",
    "        print(\"It's a negtive sentence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict using LSTM_RNN model\n",
    "def predict_CNN(str_1):\n",
    "    r = ' '.join([x  for x in tokenizer.tokenize(str_1) if x[0] != '@'])\n",
    "    max_string = ''.join('and ' for i in range(252))\n",
    "    test_x = feature_tokenizer_cnn.texts_to_sequences([r, max_string])\n",
    "    test_x = pad_sequences(test_x)\n",
    "    result = loaded_CnnModel.predict(np.array([test_x[0]]))\n",
    "    positive_prob = result[0][2] + result[0][3] + result[0][4]\n",
    "    negtive_prob = result[0][0] + result[0][1]\n",
    "    print(result[0])\n",
    "    print(\"Positive prob: \", positive_prob, \"Negative prob: \", negtive_prob)\n",
    "    if(positive_prob > negtive_prob):\n",
    "        print(\"It's a positive sentence\")\n",
    "    else:\n",
    "        print(\"It's a negtive sentence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare all four models together\n",
    "def model_compare(str_l):\n",
    "    print(\"Start to predict the sentiment of \\n \\\"\", str_l, \"\\\"\\n\")\n",
    "    print(\"Baseline model's prediction: \")\n",
    "    predict_baseline(str_l)\n",
    "    print(\"\\nSVM model's prediction: \")\n",
    "    predict_svm(str_l)\n",
    "    print(\"\\nLSTM RNN model's prediction: \")\n",
    "    predict_LSTM_RNN(str_l)\n",
    "    print(\"\\nCNN model's prediction:\" )\n",
    "    predict_CNN(str_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UCSD is the best\n",
      "Start to predict the sentiment of \n",
      " \" UCSD is the best \"\n",
      "\n",
      "Baseline model's prediction: \n",
      "Positive word counts:  2 Negative word counts:  1\n",
      "It's a positive sentence\n",
      "\n",
      "SVM model's prediction: \n",
      "It's a positive sentence\n",
      "\n",
      "LSTM RNN model's prediction: \n",
      "Positive prob:  0.9642895 Negative prob:  0.03571048\n",
      "It's a positive sentence\n",
      "\n",
      "CNN model's prediction:\n",
      "[1.6690162e-01 2.7074714e-07 2.6805029e-07 2.4749545e-07 8.3309758e-01]\n",
      "Positive prob:  0.8330981 Negative prob:  0.16690189\n",
      "It's a positive sentence\n"
     ]
    }
   ],
   "source": [
    "# Test for sentences \n",
    "smaller_data[0][1]\n",
    "test_string = smaller_data[15][1]\n",
    "test_string = \"UCSD is the best\"\n",
    "print(test_string)\n",
    "model_compare(test_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

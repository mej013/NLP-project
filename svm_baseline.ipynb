{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import string\n",
    "from collections import defaultdict\n",
    "from sklearn import linear_model\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = pd.read_csv(\"./Desktop/data.csv\", encoding='ISO-8859-1')\n",
    "data = file.values.tolist()\n",
    "\n",
    "master_data = []\n",
    "for l in data:\n",
    "    master_data.append([l[0], l[5]])\n",
    "random.shuffle(master_data)\n",
    "\n",
    "smaller_data_punc = []\n",
    "smaller_data = []\n",
    "# without punctuation \n",
    "punctuation = set(string.punctuation)\n",
    "counter = 0\n",
    "for l in master_data[:300000]:\n",
    "    r = ''.join([c for c in l[1].lower() if not c in punctuation and c !='@'])\n",
    "    if(l[0] == 0):\n",
    "        smaller_data.append([0, r])\n",
    "    else:\n",
    "        smaller_data.append([1, r])\n",
    "    counter +=1\n",
    "# with punctuation \n",
    "counter = 0\n",
    "for l in master_data[:300000]:\n",
    "    r = ''.join([c for c in l[1].lower() if c !='@'])\n",
    "    if(l[0] == 0):\n",
    "        smaller_data_punc.append([0, r])\n",
    "    else:\n",
    "        smaller_data_punc.append([1, r])\n",
    "    counter +=1\n",
    "    \n",
    "train_text = [x[1] for x in smaller_data[:200000]]\n",
    "\n",
    "wordCount = defaultdict(int)\n",
    "for l in train_text:\n",
    "    l = l.split()\n",
    "    if(len(l) == 0):\n",
    "        continue\n",
    "    wordCount[l[0]] += 1\n",
    "    for i in range(1, len(l)):\n",
    "        wordCount[l[i]] += 1\n",
    "\n",
    "counts = [(wordCount[w], w) for w in wordCount]\n",
    "counts.sort()\n",
    "counts.reverse()\n",
    "\n",
    "words = [x[1] for x in counts[:2000]]\n",
    "\n",
    "\n",
    "unigram_frequency = defaultdict(float)\n",
    "total_count = sum([i[0] for i in counts[:2000]])\n",
    "for i in counts[:2000]:\n",
    "    unigram_frequency[i[1]] = i[0]/total_count \n",
    "    \n",
    "tweet = TweetTokenizer()\n",
    "\n",
    "\n",
    "wordCountP = defaultdict(int)\n",
    "for d in smaller_data_punc:\n",
    "    review = d[1]\n",
    "    tokens = tweet.tokenize(review)\n",
    "    for w in tokens:\n",
    "        wordCountP[w] += 1\n",
    "\n",
    "countsP = [(wordCountP[w], w) for w in wordCountP]\n",
    "countsP.sort()\n",
    "countsP.reverse()\n",
    "\n",
    "wordsP = [x[1] for x in countsP[:2000]]\n",
    "\n",
    "wordIdP = dict(zip(wordsP, range(len(wordsP))))\n",
    "wordSetP = set(wordsP)\n",
    "\n",
    "\n",
    "# Calculating the most 2000 frequenct words frequncy in probalility in positive sentiment text\n",
    "unigram_frequency_pos = defaultdict(float)\n",
    "train_text_pos = [x[1] for x in smaller_data[:200000] if x[0] == 1]\n",
    "for l in train_text_pos:\n",
    "    l = l.split()\n",
    "    if(len(l) == 0):\n",
    "        continue\n",
    "    for w in l:\n",
    "        if w in words:\n",
    "            unigram_frequency_pos[w] += 1\n",
    "\n",
    "\n",
    "total_count = 0\n",
    "for w in unigram_frequency_pos:\n",
    "    total_count += unigram_frequency_pos[w]\n",
    "for w in unigram_frequency_pos:\n",
    "    unigram_frequency_pos[w] = unigram_frequency_pos[w]/total_count\n",
    "positive_words = []\n",
    "for w in unigram_frequency:\n",
    "    positive_words.append([unigram_frequency_pos[w] - unigram_frequency[w], w])\n",
    "positive_words.sort()\n",
    "positive_words.reverse()\n",
    "pos_words = [x[1] for x in positive_words][:200]\n",
    "\n",
    "\n",
    "unigram_frequency_neg = defaultdict(float)\n",
    "train_text_neg = [x[1] for x in smaller_data[:200000] if x[0] == 0]\n",
    "for l in train_text_neg:\n",
    "    l = l.split()\n",
    "    if(len(l) == 0):\n",
    "        continue\n",
    "    for w in l:\n",
    "        if w in words:\n",
    "            unigram_frequency_neg[w] += 1\n",
    "total_count = 0\n",
    "for w in unigram_frequency_neg:\n",
    "    total_count += unigram_frequency_neg[w]\n",
    "for w in unigram_frequency_neg:\n",
    "    unigram_frequency_neg[w] = unigram_frequency_neg[w]/total_count\n",
    "negative_words = []\n",
    "for w in unigram_frequency:\n",
    "    negative_words.append([unigram_frequency_neg[w] - unigram_frequency[w], w])\n",
    "negative_words.sort()\n",
    "negative_words.reverse()\n",
    "neg_words = [x[1] for x in negative_words][:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline Model\n",
    "def baseFeature(datum):\n",
    "    positive_count = 0\n",
    "    negative_count = 0\n",
    "    r = ''.join([c for c in datum[1].lower() if not c in punctuation])\n",
    "    for w in r.split():\n",
    "        if w in pos_words:\n",
    "            positive_count += 1\n",
    "        if w in neg_words:\n",
    "            negative_count += 1\n",
    "    if(negative_count > positive_count):\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_baseline(str_1):\n",
    "    base_feature = baseFeature(str_1)\n",
    "    if base_feature == 1:\n",
    "        print(\"It's positive\")\n",
    "    else:\n",
    "        print(\"It's negative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=10, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=5000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using unigram features with punctuation \n",
    "def feature3(datum):\n",
    "    feat = [0]*len(wordsP)\n",
    "    review = datum[1]\n",
    "    tokens = tweet.tokenize(review)\n",
    "    for w in tokens:\n",
    "        if w in wordsP:\n",
    "            feat[wordIdP[w]] += 1\n",
    "    feat.append(1) #offset\n",
    "    return feat\n",
    "X = [feature3(d) for d in smaller_data_punc]\n",
    "y = [d[0] for d in smaller_data_punc]\n",
    "clf = svm.LinearSVC(C=10,  max_iter=5000)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_svm(str_1):\n",
    "    datum = [0,str_1]\n",
    "    feat = []\n",
    "    feat.append(feature3(datum))\n",
    "    if clf.predict(feat)[0] == 0:\n",
    "        print(\"It's negative\")\n",
    "    else:\n",
    "        print(\"It's positive\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
